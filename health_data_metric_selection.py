# -*- coding: utf-8 -*-
"""health_data_metric_selection.py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a0tM5ciQCYQHbEArTF7Ww_OiQc9PY7MS
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score, make_scorer
from imblearn.over_sampling import SMOTE
import scipy.stats as stats

dataset_1 = pd.read_csv('/content/Aids data.csv')
X1, y1 = dataset_1.drop(columns='infected'), dataset_1['infected']
categorical_cols_1 = X1.select_dtypes(include=['object']).columns
for col in categorical_cols_1:
    X1[col] = LabelEncoder().fit_transform(X1[col])
scaler = StandardScaler()
X1 = pd.DataFrame(scaler.fit_transform(X1), columns=X1.columns)
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42, stratify=y1)
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, solver='lbfgs'),
    "Random Forest": RandomForestClassifier(random_state=42, n_estimators=100, criterion='gini'),
    "Decision Tree": DecisionTreeClassifier(random_state=42, criterion='gini'),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "KNN": KNeighborsClassifier(n_neighbors=5, algorithm='auto')
}
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1': make_scorer(f1_score),
    'mcc': make_scorer(matthews_corrcoef),
    'roc_auc': make_scorer(roc_auc_score)
}
for column in X1.columns:
    plt.figure(figsize=(8, 4))
    sns.histplot(X1[column], kde=True)
    plt.title(f"Gaussian Distribution of Feature: {column}")
    plt.xlabel(column)
    plt.ylabel("Density")
    plt.show()
performance_results = []
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
for name, model in models.items():
    scores = {metric: cross_val_score(model, X1_train, y1_train, cv=kfold, scoring=scoring[metric]) for metric in scoring}
    performance_results.append([
        name,
        np.mean(scores['accuracy']), np.std(scores['accuracy']),
        np.mean(scores['precision']), np.std(scores['precision']),
        np.mean(scores['recall']), np.std(scores['recall']),
        np.mean(scores['f1']), np.std(scores['f1']),
        np.mean(scores['mcc']), np.std(scores['mcc']),
        np.mean(scores['roc_auc']), np.std(scores['roc_auc'])
    ])
performance_df = pd.DataFrame(performance_results, columns=[
    'Model',
    'Mean Accuracy', 'Std Accuracy',
    'Mean Precision', 'Std Precision',
    'Mean Recall', 'Std Recall',
    'Mean F1', 'Std F1',
    'Mean MCC', 'Std MCC',
    'Mean AUC-ROC', 'Std AUC-ROC'
])
print("Table 2: Cross-Validation Performance Metrics")
print(performance_df)
sns.barplot(data=performance_df, x='Model', y='Mean AUC-ROC')
plt.title("Model Performance Across Metrics (Dataset 1)")
plt.show()
thresholds = np.arange(0.1, 1.0, 0.1)
sensitivity_results = {'Threshold': thresholds}
for name, model in models.items():
    model.fit(X1_train, y1_train)
    f1_scores, mcc_scores, auc_scores = [], [], []
    probs = model.predict_proba(X1_test)[:, 1]
    for threshold in thresholds:
        preds = (probs >= threshold).astype(int)
        f1_scores.append(f1_score(y1_test, preds))
        mcc_scores.append(matthews_corrcoef(y1_test, preds))
        auc_scores.append(roc_auc_score(y1_test, probs))
    sensitivity_results[f"{name} F1"] = f1_scores
    sensitivity_results[f"{name} MCC"] = mcc_scores
    sensitivity_results[f"{name} AUC-ROC"] = auc_scores
sensitivity_df = pd.DataFrame(sensitivity_results)
sensitivity_df.plot(x='Threshold', title="Threshold Sensitivity Analysis (F1, MCC, & AUC-ROC)")
plt.show()
X1_noisy = X1_train + np.random.normal(0, 0.1, X1_train.shape)
robustness_scores = []
for name, model in models.items():
    model.fit(X1_noisy, y1_train)
    preds = model.predict(X1_test)
    probs = model.predict_proba(X1_test)[:, 1]
    f1 = f1_score(y1_test, preds)
    mcc = matthews_corrcoef(y1_test, preds)
    auc = roc_auc_score(y1_test, probs)
    accuracy = accuracy_score(y1_test, preds)
    precision = precision_score(y1_test, preds)
    recall = recall_score(y1_test, preds)
    robustness_scores.append([name, f1, mcc, auc, accuracy, precision, recall])
robustness_df = pd.DataFrame(robustness_scores, columns=[
    'Model', 'F1 (Noisy)', 'MCC (Noisy)', 'AUC-ROC (Noisy)',
    'Accuracy (Noisy)', 'Precision (Noisy)', 'Recall (Noisy)'
])
fig, axs = plt.subplots(3, 2, figsize=(14, 12))
metrics = ['F1 (Noisy)', 'MCC (Noisy)', 'AUC-ROC (Noisy)', 'Accuracy (Noisy)', 'Precision (Noisy)', 'Recall (Noisy)']
titles = ['F1 Score under Noise', 'MCC under Noise', 'AUC-ROC under Noise', 'Accuracy under Noise', 'Precision under Noise', 'Recall under Noise']
for i, ax in enumerate(axs.flat):
    sns.barplot(data=robustness_df, x='Model', y=metrics[i], ax=ax)
    ax.set_title(f"Robustness Analysis of {titles[i]}")
    ax.set_ylim(0, 1)
plt.tight_layout()
plt.show()
print("Table 3: Threshold Sensitivity for F1, MCC, and AUC-ROC")
print(sensitivity_df)
p_value_f1_mcc = stats.ttest_ind(sensitivity_df["Random Forest F1"], sensitivity_df["Random Forest MCC"]).pvalue
p_value_roc_f1 = stats.ttest_ind(sensitivity_df["Random Forest F1"], sensitivity_df["Random Forest AUC-ROC"]).pvalue
print("Table 4: P-value for Statistical Comparison between F1 and MCC:", p_value_f1_mcc)
print("Table 4: P-value for Statistical Comparison between F1 and AUC-ROC:", p_value_roc_f1)
print("Table 5: Metric Robustness to Noise")
print(robustness_df)